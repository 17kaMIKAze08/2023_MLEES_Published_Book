{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fe2dc8",
   "metadata": {},
   "source": [
    "# Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55ed3c",
   "metadata": {},
   "source": [
    "A brief introduction to classification and regression tacks in Machine Learning\n",
    "\n",
    "Learning objectives:\n",
    "\n",
    "1. Distinguish classification from regression\n",
    "2. Define a loss/cost function \n",
    "3. Understand how to train a logistic/softmax regression for binary/multiclass classification\n",
    "4. Know how to benchmark a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acade3",
   "metadata": {},
   "source": [
    "## Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaef07b",
   "metadata": {},
   "source": [
    "In machine learning, classification is a supervised learning task where the model learns to assigning predefined labels, categories or classes to instances, individual data points or examples in a dataset. There is two types of classification binary and multiclass classification, respectively for classifying into two classes or multiple classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8794b",
   "metadata": {},
   "source": [
    "Here are some practical application of classification in Environmental Sciences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea40032",
   "metadata": {},
   "source": [
    "> 1. Wildlife Identification: Classification techniques can be used to identify animal species from images or audio recordings, supporting wildlife monitoring projects.\n",
    "> 2. Land Cover Classification: Satellite imagery can be classified into various land cover types, aiding in monitoring land use changes over time.\n",
    "> 3. Invasive Species Detection: Developing models that classify invasive species in images, helping conservationists identify and manage ecological threats.\n",
    "> 4. Water Quality Assessment: Using classification algorithms to determine the quality of water bodies based on factors like chemical concentrations and biological indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b52f7a9",
   "metadata": {},
   "source": [
    "**Binary Classification**: A type of classification task where there is only two possible classes, labels or categories (e.g., spam vs. non-spam emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321e5bd",
   "metadata": {},
   "source": [
    "### Evaluating Classification Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23075c64",
   "metadata": {},
   "source": [
    "**Cross-Validation:** Sampling the data and producing folds for each class, predictions are then made on the test folds and then the number of correct predictions is counted, and the final outputs is the ratio of correct predictions. Accuracy, ratio of correct predictions on the total number of instances, is not the preferred performance measure when dealing with skewed datasets (i.e., when some classes are much more frequent than others).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac31ac",
   "metadata": {},
   "source": [
    "**Confusion Matrix:**\n",
    "Summarises the model’s performance, showing true positives, true negatives, false positives, and false negatives. The rows represent the actual class, while the column represents the predicted class, the confusion matrix compares those two values. A perfect classifier would have only true positives and true negatives, so only nonzero values on its main diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8b0ba",
   "metadata": {},
   "source": [
    "![confusion.jpeg](https://miro.medium.com/v2/resize:fit:1218/format:webp/1*jMs1RmSwnYgR9CsBw-z1dw.png)\n",
    "\n",
    "[Medium : What is a confusion matrix?](https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e9982",
   "metadata": {},
   "source": [
    "**Precision and Recall:**\n",
    "\n",
    "A condensed metric that can be used is the accuracy of positive predictions, the precision, = TP / TP + FP where TP is the number of true positives, and FP is the number of false positives, if no FP than 100% precision. It is usually combined with another metric called recall, = TP / TP + FN, with FN being false negative. These two metrics are combined into a single metric called F1 score, corresponding to the harmonic mean of precision and recall, enabling a simple classifiers comparison.\n",
    "The classifier will only get a high F1 score if both recall and precision are high, but a trade-off exists between these two, one is won at the expense of the other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e843b",
   "metadata": {},
   "source": [
    "**ROC Curve:**\n",
    "\n",
    "It a plot of the recall versus the false positive rate (FPR). A trade-off exists also between the two. The comparison between classifier is achieved by measuring the area under the curve (AUC), a perfect classifier will have an AUC of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7123a08",
   "metadata": {},
   "source": [
    "### Classification Algorithm:\n",
    "Some algorithms work only with binary classifiers (e.g., SGD Classifiers, Support Vector Machine classifiers) others like Logistic Regressions classifiers (e.g., Random Forest classifiers, naïve Bayes classifiers) work on both multiple and binary classification.\n",
    "\n",
    "To overcome this obstacle, binary classifiers can use two different strategy, one-versus-the-rest (OvR) strategy and the one-versus-one (OvO) strategy.\n",
    "\n",
    "- One-Versus-the-Rest (OvR) Strategy: Train a binary classifier, for each class. When classifying, obtain scores from all classifiers and select the class with the highest score.\n",
    "- One-Versus-One (OvO) Strategy: Train a binary classifier for every pair of class. This requires N × (N – 1) / 2 classifier for N classes. To classify an image, run it through all 45 classifiers and determine the winning class. OvO is advantageous as each classifier focuses on distinguishing only two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dca454",
   "metadata": {},
   "source": [
    "### Multiclass classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90b528",
   "metadata": {},
   "source": [
    "Also known as multinomial classification, refers to a classification problem where instances are categorized into three or more distinct classes. \n",
    "Example: Classifying images of animals into categories like \"dog,\" \"cat,\" \"elephant,\" and \"lion.\"\n",
    "\n",
    "> Multilabel Classification:\n",
    "Multilabel classification deals with instances that can belong to multiple classes simultaneously. In other words, an instance can have multiple labels associated with it. \n",
    "Example: Tagging a news article with multiple categories like \"politics,\" \"economy,\" and \"technology\" to capture its diverse content.\n",
    "\n",
    ">Multioutput Classification (or Multioutput Regression):\n",
    "Multioutput classification (or regression) involves predicting multiple output variables simultaneously for each instance. Each output variable can have multiple possible values or classes. \n",
    "Example: Predicting both the color and size of a piece of fruit, where color could be \"red,\" \"green,\" or \"yellow,\" and size could be \"small,\" \"medium,\" or \"large.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955c580",
   "metadata": {},
   "source": [
    "### Training a model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f38675",
   "metadata": {},
   "source": [
    "**Cost Function :**\n",
    "When using a model, you need to define the parameters and in order to estimate the best values for these parameters you need to specify a performance measure either a utility function that measures how good the model or a cost function that quantifies how well a machine learning model’s predictions align with the actual target values. It measures the discrepancy between the predicted values generated by the model and the true values from the training dataset. The objective of a machine learning algorithm is to minimize this cost function, which essentially means improving the model’s accuracy and precision in making predictions.\n",
    " \n",
    "The choice of a cost function depends on the nature of the problem—whether it is a classification, regression, or other type of task—and the desired properties of the model’s predictions. Different algorithms and tasks require different types of cost functions.\n",
    "\n",
    "**Types of Cost Functions:**\n",
    "\n",
    "> Mean Squared Error (MSE): Used in regression tasks, it calculates the average squared difference between predicted and actual values. It penalizes larger errors more heavily.\n",
    "\n",
    "> Log Loss: Commonly used in classification tasks, especially in logistic regression and neural networks. It measures the dissimilarity between predicted probabilities and actual binary class labels.\n",
    "\n",
    "> Absolute Error (L1 Loss): Similar to MSE, but it computes the absolute difference between predicted and actual values. It is less sensitive to outliers compared to MSE.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bff76",
   "metadata": {},
   "source": [
    "**Gradient Descent:**\n",
    " A generic optimization technique that modifies the model parameters to minimize the cost function and ultimately determine the optimal model parameters, thereby rely on the partial derivative of the cost function with respect to the model parameters to iteratively improve those parameters in the direction that reduces the cost. An important parameter of Gradient Descent is the size of the steps knows as the learning rate hyperparameter, it has an impact on the number of iterations and therefor the speed of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071be627",
   "metadata": {},
   "source": [
    "> Hyperparameter is a parameter of a learning algorithm, it is not affected by the learning process and remains constant during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05985139",
   "metadata": {},
   "source": [
    "![gradient.png](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/06/12190659/4-2.png)\n",
    "\n",
    "[An Easy Guide to Gradient Descent in Machine Learning, Great Learning.](https://www.mygreatlearning.com/blog/gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd51929",
   "metadata": {},
   "source": [
    "The optimization techniques using Gradient Descent include the following approaches:\n",
    "\n",
    "> Batch Gradient Descent: Updates model parameters using the entire training dataset in each iteration, it makes it very slow if the set is heavy.\n",
    "\n",
    "> Stochastic Gradient Descent (SGD): Updates model parameters based on a single random training instance or a small batch of instances. It has a faster convergence than the previous one, but noisy updates and may not reach the global minimum.\n",
    "\n",
    "> Mini-batch Gradient Descent: A compromise between batch and SGD, updating parameters using a small batch of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b044b1",
   "metadata": {},
   "source": [
    "**Early Stopping**: a regularisation method employed in iterative learning algorithm (e.g., Gradient Descent) It involves stopping the training procedure the moment the validation error reaches its lowest point, making it a valuable tool for mitigating overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06007d",
   "metadata": {},
   "source": [
    "**Training a Binary Classifier: Logistic Regression**\n",
    "\n",
    "Logistic Regression is a popular model for binary classification, it estimates the probability of an instance to belong to a class. Its training consists of iteratively optimize the model’s parameters using gradient descent.  Training this model involves finding the parameter that enhance high probabilities estimates for positive instances and vice versa for negative instances. This is achieved using the log loss cost function. The training implies to use the partial derivatives of the previous mention cost function and a Gradient Descent algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3310d2",
   "metadata": {},
   "source": [
    "**Training a Multiclass Classifier:**\n",
    "\n",
    "Softmax Regression also called Multinominal Logistic Regression is a model based on the logistic regression that can be used for multiple classes. It used the same principle as the previous mention model, implying the estimation of probabilities for each class for a given instance using the Softmax function. It can predict only one class at a time it is multiclass exclusively not multiouput. The training for this model is based on the Cross entropy cost function and a Gradient Descent algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d435b94",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "On both a binary and a multiclass classification model involves an examination of misclassified instances, aiming to uncover valuable insights into the model's weaknesses and potential data-related issues. This process often begins with a visualisation of the Confusion Matrix, which provides a comprehensive view of common confusion patterns and helps us understand how these patterns directly influence the model's overall performance and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70775e81",
   "metadata": {},
   "source": [
    "### Other Classification Methods:\n",
    "\n",
    ">Support Vector Machines (SVM): Effective for both binary and multiclass classification tasks.\n",
    "\n",
    "> Decision Trees: Tree-like structures used for classification, providing interpretable decision rules.\n",
    "\n",
    "> Random Forests: Ensemble methods that combine multiple decision trees for improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3437e94",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174e1f4",
   "metadata": {},
   "source": [
    "In machine learning, regression is supervised learning task where the model predicts a continuous numerical value, or a real-valued output based on single or multiple input data and learn the relationship between the input features and the target variable.\n",
    "\n",
    "Here are some practical applications of classification in Environmental Sciences!\n",
    "\n",
    "> 1. Climate Modelling: models that predict climate variables such as temperature, precipitation, and sea-level rise taking into account gas concentrations, solar radiation to make long-term climate predictions.\n",
    "> 2. Air Quality Prediction: models use meteorological data, emissions data to forecast air quality levels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a4003",
   "metadata": {},
   "source": [
    "### Linear Regression:\n",
    "Linear regression is a supervised machine learning algorithm used for predicting a continuous numerical output based on one or more input features. It assumes a linear relationship between the inputs and the target variable. The goal of linear regression is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the difference between the predicted values and the actual target values. The most common cost function used in linear regression is the Mean Squared Error (MSE).\n",
    "\n",
    "### Multiple Linear Regression:\n",
    "Multiple linear regression is an extension of linear regression that deals with multiple input features. Instead of just one input, there are multiple independent variables influencing the target variable. The algorithm estimates the coefficients for each feature, determining their individual impact on the target variable while considering their interrelationships.\n",
    "\n",
    "### Other Regression Methods:\n",
    "\n",
    "> Polynomial Regression: This type of regression extends linear regression to capture nonlinear relationships by introducing polynomial terms of the input features. It fits a curve to the data instead of a straight line.\n",
    "\n",
    "> Ridge Regression (L2 Regularization): Ridge regression adds a regularization term to the linear regression cost function. It helps prevent overfitting by penalizing large coefficient values, thus promoting simpler models.\n",
    "\n",
    ">  Lasso Regression (L1 Regularization): Similar to ridge regression, lasso regression also adds a regularization term. However, it uses the absolute values of coefficients, often resulting in some coefficients being exactly zero. This leads to feature selection.\n",
    "\n",
    ">  Elastic Net Regression: Elastic Net combines L1 and L2 regularization to balance the strengths of both. It can handle situations where there are correlated features.\n",
    "\n",
    ">  Support Vector Regression (SVR): SVR applies the principles of support vector machines to regression problems. It aims to fit a hyperplane that captures as many instances within a specified margin as possible.\n",
    "\n",
    ">  Decision Tree Regression: Similar to classification decision trees, decision tree regression predicts a continuous target value by partitioning the feature space into regions and assigning the average target value of instances within each region.\n",
    "\n",
    ">  Random Forest Regression: An ensemble method combining multiple decision tree regressors. It improves predictive accuracy and reduces overfitting by averaging the predictions of individual trees.\n",
    "\n",
    ">  Gradient Boosting Regression: A boosting technique that builds an additive model in a forward stage-wise manner. It combines the predictions of weak learners (often decision trees) to create a strong predictive model.\n",
    "\n",
    "\n",
    "Each regression method has its own strengths, weaknesses, and applicability to different types of data and problem domains. The choice of which method to use depends on the nature of the data, the problem's requirements, and the desired level of interpretability and predictive accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
